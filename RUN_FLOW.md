# ClimaGuard Execution Flow

This document explains the correct order to run the ClimaGuard system files.

## ðŸ“‹ Prerequisites (One-Time Setup)

Before running anything, complete these steps:

1. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

2. **Setup database:**
   ```bash
   mysql -u your_user -p < sql/schema.sql
   ```

3. **Configure environment:**
   ```bash
   cp env.example .env
   # Edit .env with your actual credentials
   ```

---

## ðŸ”„ Daily Pipeline Flow

### **Step 1: Ingest Raw Data** â†’ `src/ingest.py`

**Purpose:** Fetch current weather and air quality data from OpenWeatherMap API and store in `weather_raw` table.

**Command:**
```bash
python src/ingest.py Toronto
```

**What it does:**
- Fetches current weather for the specified city
- Fetches air quality data
- Stores everything in `weather_raw` table

**Run this:**
- Once per day (or multiple times for different cities)
- Before running `features.py`

**Output:** Data in `weather_raw` table

---

### **Step 2: Process Features** â†’ `src/features.py`

**Purpose:** Aggregate raw data into daily features and compute risk levels.

**Command:**
```bash
python src/features.py
```

**What it does:**
- Reads from `weather_raw` table
- Groups by city and date
- Computes daily aggregates (min/avg temp, wind speed, humidity, AQI)
- Calculates wind chill
- Computes risk levels (Low/Moderate/High)
- Stores in `weather_daily` table

**Run this:**
- After `ingest.py` (can run multiple times, handles duplicates)
- Once per day after data ingestion

**Output:** Data in `weather_daily` table

---

### **Step 3: Train Models** â†’ `src/train.py`

**Purpose:** Train machine learning models on historical daily data.

**Command:**
```bash
python src/train.py
```

**What it does:**
- Reads from `weather_daily` table
- Trains Logistic Regression model
- Trains XGBoost model
- Saves models to `models/` directory:
  - `logistic_regression.joblib`
  - `scaler.joblib`
  - `xgboost.joblib`
  - `risk_mapping.joblib`
  - `feature_names.joblib`

**Run this:**
- After you have at least 2-3 days of data in `weather_daily`
- Periodically (weekly/monthly) to retrain with new data
- Before starting the API server

**Requirements:**
- Need at least 2-3 days of data with different risk levels
- More data = better model performance

**Output:** Trained models in `models/` directory

---

### **Step 4: Start API Server** â†’ `src/service.py`

**Purpose:** Start FastAPI server to serve predictions via REST API.

**Command:**
```bash
uvicorn src.service:app --reload --host 0.0.0.0 --port 8000
```

**Or:**
```bash
python src/service.py
```

**What it does:**
- Loads trained models from `models/` directory
- Starts FastAPI server on port 8000
- Provides `/risk` and `/history` endpoints
- Can make predictions in real-time

**Run this:**
- After models are trained
- Keep running while you want to use the API
- Can run continuously in production

**Endpoints:**
- `http://localhost:8000/risk?city=Toronto` - Get risk prediction
- `http://localhost:8000/history?city=Toronto&days=30` - Get history
- `http://localhost:8000/health` - Health check

---

### **Step 5: Open Dashboard** â†’ `web/index.html`

**Purpose:** Visual dashboard to view predictions and history.

**How to use:**
1. Open `web/index.html` in a web browser
2. Or serve it:
   ```bash
   cd web
   python -m http.server 8080
   ```
3. Open `http://localhost:8080` in browser

**What it does:**
- Calls FastAPI endpoints
- Displays risk predictions
- Shows historical charts and data

**Run this:**
- After API server is running
- Anytime you want to view the dashboard

---

## ðŸ“Š Complete Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Setup (Once)  â”‚
â”‚ 1. Install deps  â”‚
â”‚ 2. Setup DB     â”‚
â”‚ 3. Create .env   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Daily Pipeline â”‚
â”‚                 â”‚
â”‚  Step 1:        â”‚
â”‚  ingest.py      â”‚â”€â”€â”€â–º weather_raw table
â”‚  (fetch data)   â”‚
â”‚                 â”‚
â”‚  Step 2:        â”‚
â”‚  features.py    â”‚â”€â”€â”€â–º weather_daily table
â”‚  (aggregate)    â”‚
â”‚                 â”‚
â”‚  Step 3:        â”‚
â”‚  train.py       â”‚â”€â”€â”€â–º models/*.joblib
â”‚  (train models) â”‚
â”‚                 â”‚
â”‚  Step 4:        â”‚
â”‚  service.py     â”‚â”€â”€â”€â–º FastAPI server
â”‚  (start API)    â”‚
â”‚                 â”‚
â”‚  Step 5:        â”‚
â”‚  index.html     â”‚â”€â”€â”€â–º Dashboard UI
â”‚  (view results) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸš€ Quick Start (First Time)

**Complete setup and run once:**

```bash
# 1. Setup
pip install -r requirements.txt
cp env.example .env
# Edit .env with your credentials
mysql -u user -p < sql/schema.sql

# 2. Collect initial data (run multiple times for different cities/days)
python src/ingest.py Toronto
python src/ingest.py "New York"
python src/ingest.py London

# 3. Process features
python src/features.py

# 4. Train models (need at least 2-3 days of data)
python src/train.py

# 5. Start API
uvicorn src.service:app --reload

# 6. Open dashboard (in another terminal)
cd web
python -m http.server 8080
# Open http://localhost:8080 in browser
```

---

## ðŸ” Daily Workflow (After Initial Setup)

**For daily operations, run in this order:**

```bash
# Morning: Collect today's data
python src/ingest.py Toronto

# Process features
python src/features.py

# (Optional) Retrain models weekly/monthly
python src/train.py

# Start/restart API (if not already running)
uvicorn src.service:app --reload
```

---

## ðŸ“ File Dependencies

```
ingest.py
  â””â”€â–º Requires: .env file, MySQL database, OpenWeatherMap API key
  â””â”€â–º Creates: weather_raw table data

features.py
  â””â”€â–º Requires: weather_raw table data
  â””â”€â–º Creates: weather_daily table data

train.py
  â””â”€â–º Requires: weather_daily table data (at least 2-3 days)
  â””â”€â–º Creates: models/*.joblib files

service.py
  â””â”€â–º Requires: models/*.joblib files, weather_daily table
  â””â”€â–º Creates: predictions table data, API endpoints

index.html
  â””â”€â–º Requires: service.py running (API server)
  â””â”€â–º Creates: Dashboard UI
```

---

## âš ï¸ Common Issues

### "No weather data found"
- **Solution:** Run `ingest.py` first

### "Models not loaded"
- **Solution:** Run `train.py` first (need data in weather_daily)

### "No training data found"
- **Solution:** Run `ingest.py` and `features.py` first, collect 2-3 days of data

### "Database connection error"
- **Solution:** Check `.env` file has correct database credentials

---

## ðŸŽ¯ Summary

**Minimum flow to get predictions:**
1. `ingest.py` â†’ Collect data
2. `features.py` â†’ Process features  
3. `train.py` â†’ Train models (after 2-3 days of data)
4. `service.py` â†’ Start API
5. `index.html` â†’ View dashboard

**Daily flow:**
1. `ingest.py` â†’ Collect today's data
2. `features.py` â†’ Update daily features
3. (Optional) `train.py` â†’ Retrain periodically
4. `service.py` â†’ Keep running

